{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "00OZzzCHt8p7",
    "outputId": "e915c9c4-37f2-4716-d62d-f588ebac7547"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dinos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dinos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dinos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import os\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import OrderedDict\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "################################################################################\n",
    "############################ DATA PREPROCESSING ################################\n",
    "################################################################################\n",
    "\n",
    "\n",
    "#transforms the data of given dataframe\n",
    "def data_preprocessing(df):\n",
    "#   remove_links(df)\n",
    "#   remove_hashtags(df)\n",
    "#   remove_mentions(df)\n",
    "#   replace_links(df)\n",
    "#   replace_hashtags(df)\n",
    "#   replace_mentions(df)\n",
    "#   replace_numbers(df)\n",
    "#   replace_upper_words(df)\n",
    "  remove_non_alpha(df)\n",
    "  to_lowercase(df)\n",
    "  lemmatize(df)\n",
    "  remove_stop_words(df)\n",
    "\n",
    "def remove_links(df):\n",
    "  df.update(df.apply(lambda x: re.sub(r'(https?:\\/\\/[A-Za-z0-9\\/.]*)|(bit.ly\\/[A-Za-z0-9\\/.]*)', '', x, flags=re.MULTILINE)))\n",
    "\n",
    "def remove_non_alpha(df):\n",
    "  df.update(df.apply(lambda x: re.sub(r'[^ a-zA-Z]', ' ', x, flags=re.MULTILINE)))\n",
    "\n",
    "def to_lowercase(df):\n",
    "  df.update(df.apply(lambda x: x.lower()))\n",
    "\n",
    "def lemmatize(df):\n",
    "  lemmatizer = WordNetLemmatizer() \n",
    "  df.update(df.apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in tokenizer(x)])))\n",
    "\n",
    "def remove_stop_words(df):\n",
    "  stop = set(stopwords.words('english'))\n",
    "  df.update(df.apply(lambda x: ' '.join([word for word in tokenizer(x) if word not in (stop)])))\n",
    "\n",
    "def remove_hashtags(df):\n",
    "  df.update(df.apply(lambda x: re.sub(r'(#[A-Za-z0-9]*)', '',x, flags=re.MULTILINE)))\n",
    "\n",
    "def remove_mentions(df):\n",
    "  df.update(df.apply(lambda x: re.sub(r'(@[A-Za-z0-9]*)', '',x, flags=re.MULTILINE)))\n",
    "  \n",
    "def replace_mentions(df):\n",
    "  df.update(df.apply(lambda x: re.sub(r'(@[A-Za-z0-9]*)', '<user>',x, flags=re.MULTILINE)))\n",
    "  \n",
    "def replace_hashtags(df):\n",
    "  df.update(df.apply(lambda x: re.sub(r'(#[A-Za-z0-9]*)', '<hashtag>',x, flags=re.MULTILINE)))\n",
    "  \n",
    "def replace_numbers(df):\n",
    "  df.update(df.apply(lambda x: re.sub(r\"[-+]?\\d*\\.\\d+|\\d+\", '<number>',x, flags=re.MULTILINE)))\n",
    "\n",
    "def replace_upper_words(df):\n",
    "  df.update(df.apply(lambda x: re.sub(r'( [A-Z]* )', ' <allcaps> ',x, flags=re.MULTILINE)))\n",
    "\n",
    "def replace_links(df):\n",
    "  df.update(df.apply(lambda x: re.sub(r'(https?:\\/\\/[A-Za-z0-9\\/.]*)|(bit.ly\\/[A-Za-z0-9\\/.]*)', '<url>', x, flags=re.MULTILINE)))\n",
    "\n",
    "\n",
    "################################################################################\n",
    "############################## UTILITY FUNCTIONS ###############################\n",
    "################################################################################\n",
    "\n",
    "def set_seed(seed = 1234):\n",
    "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
    "    This is for REPRODUCIBILITY.'''\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "\n",
    "#a tokenizer that recognizes the special_tokens list as tokens\n",
    "def tokenizer(input_str):\n",
    "    final_tokens = []\n",
    "    special_tokens = ['<user>','<allcaps>','<hashtag>','<number>','<url>']\n",
    "    for token in special_tokens:\n",
    "        for i in range(input_str.count(token)):\n",
    "            final_tokens.append(token)\n",
    "        input_str = input_str.replace(token,'')\n",
    "    final_tokens += word_tokenize(input_str)\n",
    "    return final_tokens\n",
    "\n",
    "\n",
    "#load glove model\n",
    "#got ideas from https://stackoverflow.com/questions/37793118/load-pretrained-glove-vectors-in-python\n",
    "def load_pre_trained_model(filename):\n",
    "  pretrained_model=dict()\n",
    "  with open(filename,'r') as fd:\n",
    "    for line in fd:\n",
    "      tokens = line.split()\n",
    "      pretrained_model[tokens[0]]=np.array(tokens[1:],dtype=np.float32)\n",
    "\n",
    "  return pretrained_model\n",
    "\n",
    "\n",
    "#vectorize df using word embeddings\n",
    "def vectorize(df,vocabulary):\n",
    "  document_embeddings = df.apply(\n",
    "      lambda x: np.array([vocabulary[word] for word in tokenizer(x) if word in vocabulary.keys()])\n",
    "  )\n",
    "  return document_embeddings\n",
    "\n",
    "\n",
    "def ROCplot(testy,predy):\n",
    "    classesc=[0,1,2]\n",
    "    test_y = label_binarize(testy,classes=classesc)\n",
    "    pred_y = label_binarize(predy,classes=classesc)\n",
    "    n_classes=3\n",
    "\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(test_y[:, i], pred_y[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    # Plot all ROC curves\n",
    "    plt.figure()\n",
    "    colors = ['red','blue', 'green']\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color,\n",
    "                 label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                 ''.format(classesc[i], roc_auc[i]))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def learning_curve_plot(epochs, train_scores, test_scores, yaxis='Loss'):\n",
    "    plt.grid()\n",
    "    plt.title(\"Learning curve Result\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(yaxis)\n",
    "    plt.fill_between(\n",
    "        epochs,\n",
    "        train_scores,\n",
    "        alpha=0.1,\n",
    "        color=\"r\",\n",
    "    )\n",
    "    plt.fill_between(\n",
    "        epochs,\n",
    "        test_scores,\n",
    "        alpha=0.1,\n",
    "        color=\"g\",\n",
    "    )\n",
    "    plt.plot(\n",
    "        epochs, train_scores, \"o-\", color=\"r\", label=\"Training score\"\n",
    "    )\n",
    "    plt.plot(\n",
    "        epochs, test_scores, \"o-\", color=\"g\", label=\"Validation score\"\n",
    "    )\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#returns a dataframe holding precision,recall,f1,accurasy scores\n",
    "#test on training and validation sets\n",
    "def get_scores(trainTrue,trainPred,validationTrue,validationPred):\n",
    "\n",
    "    scores={}\n",
    "    scores['set']=[]\n",
    "    scores['precision']=[]\n",
    "    scores['f1']=[]\n",
    "    scores['recall']=[]\n",
    "    scores['accuracy']=[]\n",
    "\n",
    "    scores['set'].append('Training')\n",
    "    scores['precision'].append(metrics.precision_score(trainTrue,trainPred,average='weighted',zero_division=0))\n",
    "    scores['f1'].append(metrics.f1_score(trainTrue,trainPred,average='weighted',zero_division=0))\n",
    "    scores['recall'].append(metrics.recall_score(trainTrue,trainPred,average='weighted',zero_division=0))\n",
    "    scores['accuracy'].append(metrics.accuracy_score(trainTrue,trainPred))\n",
    "\n",
    "    scores['set'].append('Validation')\n",
    "    scores['precision'].append(metrics.precision_score(validationTrue,validationPred,average='weighted',zero_division=0))\n",
    "    scores['f1'].append(metrics.f1_score(validationTrue,validationPred,average='weighted',zero_division=0))\n",
    "    scores['recall'].append(metrics.recall_score(validationTrue,validationPred,average='weighted',zero_division=0))\n",
    "    scores['accuracy'].append(metrics.accuracy_score(validationTrue,validationPred))\n",
    "\n",
    "    return  pd.DataFrame(data=scores).set_index('set')\n",
    "\n",
    "\n",
    "def display_results(Y_train,Y_validation,Pred_train,Pred_validation,Loss_train,\n",
    "                    Loss_validation,Score_train,Score_validation,epochs):\n",
    "    display(get_scores(Y_train,Pred_train,Y_validation,Pred_validation))\n",
    "    learning_curve_plot(epochs, Loss_train, Loss_validation, yaxis='Loss')\n",
    "    learning_curve_plot(epochs, Score_train, Score_validation, yaxis='f1')\n",
    "    ROCplot(Y_validation,Pred_validation)\n",
    "\n",
    "\n",
    "################################################################################\n",
    "############################ NEURAL NETWORK ####################################\n",
    "################################################################################\n",
    "\n",
    "#got ideas from https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, feature_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layer1 = nn.LSTM(feature_size, 100, batch_first=True, bidirectional=True)\n",
    "        self.layer2 = nn.LSTM(200, 50, batch_first=True, bidirectional=True)\n",
    "        self.layer3 = nn.LSTM(feature_size, 50, batch_first=True, bidirectional=True)\n",
    "        self.layer4 = nn.GRU(100, 25, batch_first=True, bidirectional=True)\n",
    "        self.layer5 = nn.GRU(50, 5, batch_first=True, bidirectional=True)\n",
    "        self.layer6 = nn.GRU(10, 3, batch_first=True)\n",
    "        self.layer7 = nn.GRU(50, 3, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        # x = nn.BatchNorm1d(x.shape[1])(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        a = self.layer1(x)[0]\n",
    "        a = nn.BatchNorm1d(a.shape[1])(a)\n",
    "        a = self.dropout(a)\n",
    "        a = self.layer2(a)[0]\n",
    "        b = self.layer3(x)[0]\n",
    "\n",
    "        x = torch.add(a,b)\n",
    "        x = self.layer4(x)[0]\n",
    "        x = nn.BatchNorm1d(x.shape[1])(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        a = self.layer5(x)[0]\n",
    "        a = nn.BatchNorm1d(x.shape[1])(a)\n",
    "        a = self.dropout(a)\n",
    "        a = self.layer6(a)[0]\n",
    "\n",
    "        b = self.layer7(x)[0]\n",
    "\n",
    "        x = torch.add(a,b)\n",
    "        return x.mean(dim=1)\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx],dtype=torch.float32),self.y[idx]\n",
    "\n",
    "def sequence_padding(batch):\n",
    "    (xx, yy) = zip(*batch)\n",
    "    xx = pad_sequence(xx, batch_first=True, padding_value=0)\n",
    "    return xx, torch.tensor(yy)\n",
    "\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    loss_sum, score_sum = 0,0\n",
    "\n",
    "    for X, y in dataloader:\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        score_sum += metrics.f1_score(y,pred.argmax(1),average='weighted',zero_division=0)\n",
    "        loss_sum += loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(model.parameters(),2)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss_sum/num_batches, score_sum/num_batches\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    loss_sum, score_sum = 0, 0\n",
    "    final_pred = torch.zeros(1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            final_pred = torch.cat((final_pred, pred.argmax(dim=1)),0)\n",
    "            loss_sum += loss_fn(pred, y).item()\n",
    "            score_sum += metrics.f1_score(y,pred.argmax(1),average='weighted',zero_division=0)\n",
    "\n",
    "    return loss_sum/num_batches, score_sum/num_batches, final_pred[1:]\n",
    "\n",
    "\n",
    "#init dataloader and train the model\n",
    "def run_model(X_train, Y_train, X_validation, Y_validation, batch_size, epochs,\n",
    "              lr_scheduler, loss_fn, optimizer, model):\n",
    "    Set_train = CustomDataset(X_train,Y_train)\n",
    "    Set_validation = CustomDataset(X_validation,Y_validation)\n",
    "\n",
    "    Dataloader_train = DataLoader(Set_train, batch_size=batch_size, collate_fn=sequence_padding)\n",
    "    Dataloader_validation = DataLoader(Set_validation, batch_size=batch_size, collate_fn=sequence_padding)\n",
    "\n",
    "    Loss_train=[]\n",
    "    Loss_validation=[]\n",
    "    Score_train=[]\n",
    "    Score_validation=[]\n",
    "\n",
    "    for current_epoch in range(epochs):\n",
    "        print(\"-------------------------------------------------------------------------------\")\n",
    "        print(\"Epoch \", current_epoch+1, \":\")\n",
    "        print(\"Learning rate: \", lr_scheduler.get_last_lr())\n",
    "\n",
    "        epoch_loss_train, epoch_score_train = train_loop(Dataloader_train, model, loss_fn, optimizer)\n",
    "        epoch_loss_validation, epoch_score_validation, _ = test_loop(Dataloader_validation, model, loss_fn)\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        Loss_train.append(epoch_loss_train)\n",
    "        Loss_validation.append(epoch_loss_validation)\n",
    "        Score_train.append(epoch_score_train)\n",
    "        Score_validation.append(epoch_score_validation)\n",
    "\n",
    "        print(\"Training: Loss: \", epoch_loss_train, \" Score: \", epoch_score_train)\n",
    "        print(\"Test    : Loss: \", epoch_loss_validation, \" Score: \", epoch_score_validation)\n",
    "\n",
    "    _, _, Pred_train = test_loop(Dataloader_train, model, loss_fn)\n",
    "    _, _, Pred_validation = test_loop(Dataloader_validation, model, loss_fn)\n",
    "\n",
    "    display_results(Y_train,Y_validation,Pred_train,Pred_validation,Loss_train,\n",
    "                    Loss_validation,Score_train,Score_validation,range(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4_JW47H6v33l",
    "outputId": "0487a8b2-6636-4df6-a5f9-fd9d678b7589"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "training_set_path = \"C:\\\\Users\\\\dinos\\\\Desktop\\\\vaccine_train_set.csv\"\n",
    "validation_set_path = \"C:\\\\Users\\\\dinos\\\\Desktop\\\\vaccine_validation_set.csv\"\n",
    "\n",
    "\n",
    "#set manual seeds\n",
    "set_seed()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')\n",
    "\n",
    "\n",
    "#importing datasets\n",
    "\n",
    "train = pd.read_csv(training_set_path,usecols=['tweet','label'])\n",
    "validation = pd.read_csv(validation_set_path,usecols=['tweet','label'])\n",
    "\n",
    "\n",
    "#data preprocessing\n",
    "\n",
    "data_preprocessing(train['tweet'])\n",
    "data_preprocessing(validation['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6u-AuG2IwRZS",
    "outputId": "0783dadd-a812-4451-b492-c001b572c58c",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Epoch  1 :\n",
      "Learning rate:  [0.1]\n",
      "Training: Loss:  0.9947507845163346  Score:  0.31852726886270444\n",
      "Test    : Loss:  0.9913745845357577  Score:  0.3634640436550124\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch  2 :\n",
      "Learning rate:  [0.09000000000000001]\n",
      "Training: Loss:  0.9909972252845765  Score:  0.3269253829507746\n",
      "Test    : Loss:  0.9905143173204528  Score:  0.3130411472222374\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch  3 :\n",
      "Learning rate:  [0.08100000000000002]\n",
      "Training: Loss:  0.9901564345359802  Score:  0.30468565433851846\n",
      "Test    : Loss:  0.9899065759446886  Score:  0.3145009658268488\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch  4 :\n",
      "Learning rate:  [0.07290000000000002]\n",
      "Training: Loss:  0.9901964709758758  Score:  0.36679078029194123\n",
      "Test    : Loss:  0.9889249983761046  Score:  0.4043755760072293\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch  5 :\n",
      "Learning rate:  [0.06561000000000002]\n",
      "Training: Loss:  0.9903281489610672  Score:  0.3963476568928224\n",
      "Test    : Loss:  0.989245619210932  Score:  0.3855940122881688\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch  6 :\n",
      "Learning rate:  [0.05904900000000002]\n",
      "Training: Loss:  0.9902008217573166  Score:  0.35257927469207534\n",
      "Test    : Loss:  0.9890102710988786  Score:  0.3580810026352405\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch  7 :\n",
      "Learning rate:  [0.05314410000000002]\n",
      "Training: Loss:  0.9901179324388504  Score:  0.3525483959788829\n",
      "Test    : Loss:  0.9890544298622344  Score:  0.39729781529537805\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch  8 :\n",
      "Learning rate:  [0.04782969000000002]\n",
      "Training: Loss:  0.98964224421978  Score:  0.3458478356739278\n",
      "Test    : Loss:  0.9882249550686942  Score:  0.31194266598394893\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch  9 :\n",
      "Learning rate:  [0.043046721000000024]\n",
      "Training: Loss:  0.9896437262296677  Score:  0.31174445560433744\n",
      "Test    : Loss:  0.9889485662182173  Score:  0.3082157917868488\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch  10 :\n",
      "Learning rate:  [0.03874204890000002]\n",
      "Training: Loss:  0.9891609823703766  Score:  0.30762088394775394\n",
      "Test    : Loss:  0.988637189898226  Score:  0.32105971318225796\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch  11 :\n",
      "Learning rate:  [0.03486784401000002]\n",
      "Training: Loss:  0.9895098212957382  Score:  0.306066515601643\n",
      "Test    : Loss:  0.987992352909512  Score:  0.3059853383834809\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch  12 :\n",
      "Learning rate:  [0.03138105960900001]\n",
      "Training: Loss:  0.9892758579254151  Score:  0.3047098000872655\n",
      "Test    : Loss:  0.9881504591968324  Score:  0.3121589633565096\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch  13 :\n",
      "Learning rate:  [0.028242953648100012]\n",
      "Training: Loss:  0.9884779362678527  Score:  0.304958099557872\n",
      "Test    : Loss:  0.9876972213387489  Score:  0.3068593484193124\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch  14 :\n",
      "Learning rate:  [0.025418658283290013]\n",
      "Training: Loss:  0.9888859884738922  Score:  0.305305797322237\n",
      "Test    : Loss:  0.9884299089511236  Score:  0.30988689232690914\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch  15 :\n",
      "Learning rate:  [0.022876792454961013]\n",
      "Training: Loss:  0.988771288394928  Score:  0.3053879190939175\n",
      "Test    : Loss:  0.9886782061722543  Score:  0.30510958026127644\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch  16 :\n",
      "Learning rate:  [0.020589113209464913]\n",
      "Training: Loss:  0.9886059858798981  Score:  0.30736123528459436\n",
      "Test    : Loss:  0.9858678430318832  Score:  0.30807837815660005\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch  17 :\n",
      "Learning rate:  [0.01853020188851842]\n",
      "Training: Loss:  0.9884004452228546  Score:  0.30431646565224085\n",
      "Test    : Loss:  0.9865286871790886  Score:  0.30671204152452824\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch  18 :\n",
      "Learning rate:  [0.01667718169966658]\n",
      "Training: Loss:  0.9885521782636643  Score:  0.30452534538650067\n",
      "Test    : Loss:  0.9881495593322648  Score:  0.30682345813788564\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch  19 :\n",
      "Learning rate:  [0.015009463529699923]\n",
      "Training: Loss:  0.9885408380031586  Score:  0.3024767400444272\n",
      "Test    : Loss:  0.9880819229616059  Score:  0.30917081068638397\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch  20 :\n",
      "Learning rate:  [0.013508517176729932]\n",
      "Training: Loss:  0.9882464816570282  Score:  0.30264355505519186\n",
      "Test    : Loss:  0.9878028324908681  Score:  0.30553071590190617\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch  21 :\n",
      "Learning rate:  [0.01215766545905694]\n",
      "Training: Loss:  0.9881483622789383  Score:  0.3024119276327165\n",
      "Test    : Loss:  0.9878705690304438  Score:  0.3066598192249896\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch  22 :\n",
      "Learning rate:  [0.010941898913151246]\n",
      "Training: Loss:  0.9883142173290252  Score:  0.3020975471692866\n",
      "Test    : Loss:  0.9879478183057573  Score:  0.3045288837633147\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch  23 :\n",
      "Learning rate:  [0.009847709021836121]\n",
      "Training: Loss:  0.9885743539333344  Score:  0.302441782969448\n",
      "Test    : Loss:  0.9879583112067647  Score:  0.3055554537810355\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch  24 :\n",
      "Learning rate:  [0.00886293811965251]\n",
      "Training: Loss:  0.9889674414396286  Score:  0.3024977404906324\n",
      "Test    : Loss:  0.9869563281536102  Score:  0.30570130710775456\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch  25 :\n",
      "Learning rate:  [0.007976644307687259]\n",
      "Training: Loss:  0.9889916553497314  Score:  0.3020206829279775\n",
      "Test    : Loss:  0.988252224193679  Score:  0.3039686620167521\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch  26 :\n",
      "Learning rate:  [0.007178979876918534]\n",
      "Training: Loss:  0.9886513887643814  Score:  0.3020103988176789\n",
      "Test    : Loss:  0.988242994580004  Score:  0.3037678872624059\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch  27 :\n",
      "Learning rate:  [0.006461081889226681]\n",
      "Training: Loss:  0.9880420964956284  Score:  0.30211878804062064\n",
      "Test    : Loss:  0.9881375572747655  Score:  0.3039686620167521\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch  28 :\n",
      "Learning rate:  [0.005814973700304013]\n",
      "Training: Loss:  0.9883946011066437  Score:  0.3022118598174233\n",
      "Test    : Loss:  0.9875674123565356  Score:  0.3037862826677061\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch  29 :\n",
      "Learning rate:  [0.005233476330273611]\n",
      "Training: Loss:  0.9883079829216004  Score:  0.30198203002335267\n",
      "Test    : Loss:  0.9873191209303008  Score:  0.3039686620167521\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch  30 :\n",
      "Learning rate:  [0.00471012869724625]\n",
      "Training: Loss:  0.988344094634056  Score:  0.30202820215458637\n",
      "Test    : Loss:  0.9875445862611135  Score:  0.3037716565005976\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch  31 :\n",
      "Learning rate:  [0.004239115827521626]\n",
      "Training: Loss:  0.9881727108955384  Score:  0.3023420739312818\n",
      "Test    : Loss:  0.9877577937311597  Score:  0.3039686620167521\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch  32 :\n",
      "Learning rate:  [0.0038152042447694634]\n",
      "Training: Loss:  0.9884058609008789  Score:  0.30228348612769\n",
      "Test    : Loss:  0.9872052975826793  Score:  0.30404275362730204\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch  33 :\n",
      "Learning rate:  [0.003433683820292517]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Loss:  0.9879660187959671  Score:  0.3021090345488728\n",
      "Test    : Loss:  0.9890670677026113  Score:  0.3039686620167521\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch  34 :\n",
      "Learning rate:  [0.0030903154382632653]\n",
      "Training: Loss:  0.9880283560752868  Score:  0.3021400742725047\n",
      "Test    : Loss:  0.9867851030495431  Score:  0.3039686620167521\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch  35 :\n",
      "Learning rate:  [0.002781283894436939]\n",
      "Training: Loss:  0.9881607784032822  Score:  0.30198502883882056\n",
      "Test    : Loss:  0.9865527177850405  Score:  0.3039686620167521\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch  36 :\n",
      "Learning rate:  [0.002503155504993245]\n",
      "Training: Loss:  0.9881955523490906  Score:  0.3021104289835914\n",
      "Test    : Loss:  0.9869197193119261  Score:  0.3039686620167521\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch  37 :\n",
      "Learning rate:  [0.0022528399544939205]\n",
      "Training: Loss:  0.9885950657129288  Score:  0.3020103988176789\n",
      "Test    : Loss:  0.9869659576151106  Score:  0.3039686620167521\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch  38 :\n",
      "Learning rate:  [0.0020275559590445286]\n",
      "Training: Loss:  0.9883930637836457  Score:  0.3020103988176789\n",
      "Test    : Loss:  0.9871046294768652  Score:  0.3039686620167521\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch  39 :\n",
      "Learning rate:  [0.0018248003631400759]\n",
      "Training: Loss:  0.9879633184671403  Score:  0.3020103988176789\n",
      "Test    : Loss:  0.9875996170772446  Score:  0.3039686620167521\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch  40 :\n",
      "Learning rate:  [0.0016423203268260682]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-f14262e7f1cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m run_model(X_train, Y_train, X_validation, Y_validation, batch_size, epochs,\n\u001b[1;32m---> 51\u001b[1;33m               lr_scheduler, loss_fn, optimizer, model)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-23-11bebb180294>\u001b[0m in \u001b[0;36mrun_model\u001b[1;34m(X_train, Y_train, X_validation, Y_validation, batch_size, epochs, lr_scheduler, loss_fn, optimizer, model)\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Learning rate: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_last_lr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 344\u001b[1;33m         \u001b[0mepoch_loss_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_score_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDataloader_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    345\u001b[0m         \u001b[0mepoch_loss_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_score_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDataloader_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m         \u001b[0mlr_scheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-11bebb180294>\u001b[0m in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m    298\u001b[0m         \u001b[1;31m# Backpropagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m         \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#model hyperparameters\n",
    "\n",
    "learning_rate = 0.1\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "\n",
    "\n",
    "#download the glove pre trained embeddings\n",
    "\n",
    "# !wget https://nlp.stanford.edu/data/glove.6B.zip\n",
    "# !unzip glove.6B.zip\n",
    "\n",
    "\n",
    "#load the pre-trained model\n",
    "\n",
    "# glove_pretrained_model = load_pre_trained_model(\"C:\\\\Users\\\\dinos\\\\Desktop\\\\glove.6B.300d.txt\")\n",
    "\n",
    "\n",
    "#vectorize data\n",
    "\n",
    "# X_train = vectorize(train['tweet'],glove_pretrained_model)\n",
    "# X_validation = vectorize(validation['tweet'],glove_pretrained_model)\n",
    "# Y_train,Y_validation = train['label'], validation['label']\n",
    "\n",
    "\n",
    "#initialize neural network with prerequisite layers below\n",
    "\n",
    "model = NeuralNetwork(len(X_train[0][0]))\n",
    "\n",
    "\n",
    "#Loss function: Cross entropy \n",
    "#using label smoothing to avoid minor overfitting\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "#Optimizer: SGD using momentum\n",
    "#Regularizer: L2 regularization via weight_decay\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "#LR scheduler: exponential decrease\n",
    "\n",
    "lr_scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "\n",
    "#train the model and print results\n",
    "\n",
    "run_model(X_train, Y_train, X_validation, Y_validation, batch_size, epochs,\n",
    "              lr_scheduler, loss_fn, optimizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23gAN0TSaS9G"
   },
   "outputs": [],
   "source": [
    "print(\"Model structure: \", model, \"\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ai2_project3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
